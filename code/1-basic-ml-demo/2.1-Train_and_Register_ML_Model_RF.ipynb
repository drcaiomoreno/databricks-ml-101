{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d545e8f2-0660-4b1b-a225-0c3b618049dc",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757608475754}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Query SQL"
    }
   },
   "outputs": [],
   "source": [
    "sql_df = spark.sql(\"SELECT square_feet_float, num_rooms_float, age_float, distance_to_city_km_float, price_float FROM kaggle_ml_demo.house_prediction.train_data_house_prediction_float\")\n",
    "display(sql_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b50487c-3d4e-4c92-a98e-58c612ae1be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Prepare features using the aliased columns\n",
    "feature_columns = ['square_feet_float', 'num_rooms_float', 'age_float', 'distance_to_city_km_float']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "data = assembler.transform(sql_df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price_float')\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol='price_float', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Display RMSE\n",
    "display(spark.createDataFrame([(rmse,)], ['RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2db65b-4d45-41c5-a648-4a18269b1216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set the registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Define the catalog, schema, and model name\n",
    "catalog_name = \"kaggle_ml_demo\"\n",
    "schema_name = \"house_prediction\"\n",
    "model_name = \"ml_model_house_prediction\"\n",
    "full_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "# Infer the model signature\n",
    "signature = infer_signature(train_data.select(feature_columns).toPandas(), lr_model.transform(train_data).select(\"prediction\").toPandas())\n",
    "\n",
    "# Log the model with the signature\n",
    "with mlflow.start_run(nested=True) as run:\n",
    "    mlflow.spark.log_model(\n",
    "        lr_model, \n",
    "        \"model\", \n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    # Register the model to Unity Catalog\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fc935f-1a7a-4777-93df-e8b28dcbd6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = ['square_feet_float', 'num_rooms_float', 'age_float', 'distance_to_city_km_float']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "rf_data = assembler.transform(sql_df)\n",
    "\n",
    "train_data, test_data = rf_data.randomSplit([0.8, 0.2], seed=42)\n",
    "display(train_data)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf90058a-b428-42b3-bf32-6557f4473090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='price_float', seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='price_float', predictionCol='prediction', metricName='rmse')\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "display(rf_predictions.select('features', 'price_float', 'prediction'))\n",
    "display(spark.createDataFrame([(rf_rmse,)], ['RF_RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c88432a-3efc-4657-ae43-0ac4944123de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_cols = ['square_feet_float', 'num_rooms_float', 'age_float', 'distance_to_city_km_float']\n",
    "target_col = 'price_float'  # Replace with your actual target column name\n",
    "\n",
    "# Check if \"features\" column already exists in the DataFrame\n",
    "existing_cols = train_data.columns\n",
    "if \"features\" in existing_cols:\n",
    "    train_data = train_data.drop(\"features\")\n",
    "    test_data = test_data.drop(\"features\")\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Train the RandomForestRegressor model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_col, seed=42)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Log the model and metrics using MLflow\n",
    "with mlflow.start_run(run_name=\"Basic RF Run\", nested=True) as run:\n",
    "    mlflow.spark.log_model(rf_model, \"random_forest_model\")\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    print(f\"Inside MLflow Run with run_id `{run_id}` and experiment_id `{experiment_id}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21d843d-6c9e-44e4-8732-17b2800581ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set the registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Define the catalog, schema, and model name\n",
    "catalog_name = \"kaggle_ml_demo\"\n",
    "schema_name = \"house_prediction\"\n",
    "model_name = \"ml_model_house_prediction_rf\"\n",
    "full_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "# Infer the model signature\n",
    "signature = infer_signature(train_data.select(feature_columns).toPandas(), rf_model.transform(train_data).select(\"prediction\").toPandas())\n",
    "\n",
    "# Log the model with the signature\n",
    "with mlflow.start_run(nested=True) as run:\n",
    "    mlflow.spark.log_model(\n",
    "        rf_model, \n",
    "        \"model\", \n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    # Register the model to Unity Catalog\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2feaff28-3341-4242-8faa-71e23b3713d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get feature importances from the trained RandomForest model\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "display(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25efa66f-ec97-4cd9-9f6e-2c08810dbdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Get feature importances from the trained RandomForest model\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Save feature importances to a temporary CSV file\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    csv_path = os.path.join(tmpdir, \"feature_importance.csv\")\n",
    "    feature_importance_df.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path, artifact_path=\"feature_importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2785ec00-5c36-4ad3-843d-7aa533e85de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "\n",
    "search_space = {\n",
    "    'numTrees': scope.int(hp.quniform('numTrees', 20, 100, 1)),\n",
    "    'maxDepth': scope.int(hp.quniform('maxDepth', 2, 10, 1)),\n",
    "    'maxBins': scope.int(hp.quniform('maxBins', 16, 64, 1))\n",
    "}\n",
    "\n",
    "def train_rf(params):\n",
    "    with mlflow.start_run(run_name=\"Hyperparameter Tuning RF Run\", nested=True):\n",
    "        rf = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=target_col,\n",
    "            numTrees=params['numTrees'],\n",
    "            maxDepth=params['maxDepth'],\n",
    "            maxBins=params['maxBins'],\n",
    "            seed=42\n",
    "        )\n",
    "        model = rf.fit(train_data)\n",
    "        preds = model.transform(test_data)\n",
    "        evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(preds)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=train_rf,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "display(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516dae9f-a021-4880-883b-85fa98b09eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Find the best run (lowest rmse)\n",
    "runs_df = mlflow.search_runs(order_by=['metrics.rmse ASC', 'start_time DESC'])\n",
    "best_run = runs_df.iloc[0]\n",
    "best_run_id = best_run.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec9bcb3-0693-4588-8999-64e782b826b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "artifact_uri=f\"runs:/{best_run_id}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d2ef08-435c-40af-9110-160c96d4e984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"runs:/{best_run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a76f2e-4d5b-440b-b3c3-a310ca40caa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # Register the model to Unity Catalog\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d036bf9-a901-4320-8d35-1c4822e8a90b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "End MLFlow Run"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d23763f-31ea-45a2-ae19-7731e634e205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run with UUID d18b9a9e7ad74f42b584773a66e3b742 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.1-Train_and_Register_ML_Model_RF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
